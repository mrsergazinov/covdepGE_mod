---
title: "Investigating False Positives in Covariate-Dependent Graphical Model"
author: "Isaac R, Renat S, Gözde S"
format: 
  beamer:
    include-in-header: 
      - preamble.tex
      - text: \usecolortheme[named=tamumaroon]{structure}
      - text: \setbeameroption{hide notes} 
    theme: Boadilla
    fonttheme: serif
bibliography: references.bib
---

```{r setup, include=FALSE}
# library(covdepGE)
library(tidyverse)
library(purrr)
library(Matrix)
library(xtable)
```

# Background and Introduction

## Graphical Modeling

```{=tex}
\note{
 Gozde:
 
 Undirected graphical models enables to model multivariate distributions. Suppose we observe a $p$-dimensional sample $x = (x_1, \dots , x_p)$ from a multivariate Gaussian distribution with a non-singular covariance matrix. Then the conditional independence structure of the distribution can be represented with a graph $G$. The graph $G = (V, E)$ is characterized by a node set $V = (1, \dots, p)$ corresponding to the $p$ variables, and an edge set $E$ such that $(i,j)\in E$ if and only if $x_i$ and $x_j$ are conditionally dependent given all other variable. The goal is to estimate the underlying graph $G$ from given $n$ idd observations 
$x_1, \dots , x_p$.
}
```
-   Undirected graphical models are used to model multivariate distributions.
-   Suppose we observe a $p$-dimensional sample $x = (x_1, \dots , x_p)$ from a multivariate Gaussian distribution with a non-singular covariance matrix.
-   The conditional independence structure of the distribution can be represented with a graph $G$.
-   Node set $V = (1, \dots, p)$ corresponding to the $p$ variables,
-   Edge set $E$ such that $(i,j)\in E$ if and only if $x_i$ and $x_j$ are conditionally dependent given all other variables.
-   **Goal:** estimate the underlying graph $G$ from given $n$ i.d.d. observations $x_1, \dots , x_n$.

## Adding Covariates

```{=tex}
\note{
 Gozde
 
 Several methods developed under this assumption however, in practice, the observations may not be identically distributed. In this paper they suppose the variability in the graph structure across observations depending on additional covariate information. Let $X \in \mathbb{R}^{n \times p}$ stand for the data matrix corresponding to $n$ individuals on $p$ variables. We denote the rows $X_i \in \mathbb{R}^p$ corresponding the observation for individual $i$ and the columns $x_j \in \mathbb{R}^n$.\\ The main goal of this paper is to learn the graph structure $G$ from a collection of $p$-variate independent samples $X_i$, *as a function of some extraneous covariates* $z_i$ corresponding to the samples. The only assumption on the dependence structure is that the graph parameters vary smoothly with respect to the covariates, that is, if $z_i$ and $z_j$ are similar, then the graph structure corresponding to $X_i$ and $X_j$ will be similar. 
}
```
-   The observations may not be identically distributed
-   In this paper[@origPaper] they suppose the variability in the graph structure across observations depending on additional covariate information.
-   $X \in \mathbb{R}^{n \times p}$ stands for the data matrix corresponding to $n$ individuals on $p$ variables
-   Rows $X_i \in \mathbb{R}^p$ corresponding the observation for individual $i$
-   Columns $x_j \in \mathbb{R}^n$ corresponding the observation for variable $j$

## Adding Covariates

```{=tex}
\note{
 Gozde
 
 The main goal of this paper is to learn the graph structure $G$ from a collection of $p$-variate independent samples $X_i$, *as a function of some extraneous covariates* $z_i$ corresponding to the samples. The only assumption on the dependence structure is that the graph parameters vary smoothly with respect to the covariates, that is, if $z_i$ and $z_j$ are similar, then the graph structure corresponding to $X_i$ and $X_j$ will be similar. 
}
```
-   The main goal of this paper is to learn the graph structure $G$ from a collection of $p$-variate independent samples $X_i$, *as a function of some extraneous covariates* $z_i$ corresponding to the samples.
-   The only assumption on the dependence structure is that the graph parameters vary smoothly with respect to the covariates, that is, if $z_i$ and $z_j$ are similar, then the graph structure corresponding to $X_i$ and $X_j$ will be similar.

## Existing Literature

```{=tex}
\note{
 Gozde
 
 There are several approaches to model heterogeneous graphs

\begin{itemize}
\item Without using covariate information: These methods depend on the criteria of first splitting the data into homogeneous groups and sharing information withing groups.
\item Adding the covariates into the mean structure of Gaussian graphical models as multiple linear regressions such that the mean is a continuous function of covariates. This approaches studied from a Bayesian perspective and a frequentist perspective. For this approach still uses the homogeneous graph structure for all observation which we do not want. 
\item Modeling the underlying covariance matrix as a function of the covariates. The main difficulty of this approach is to enforce sparsity in the precision matrix while being positive definite, as the sparsity in the covariance matrix does not normally carry to the precision matrix through matrix inversion.
\end{itemize}
}
```
-   Without using covariate information:
    -   These methods depend on the criteria of first splitting the data into homogeneous groups and sharing information withing groups
-   Adding the covariates into the mean structure of Gaussian graphical models as multiple linear regressions such that the mean is a continuous function of covariates.
    -   This approaches studied from a Bayesian perspective and a frequentist perspective.
    -   Still uses the homogeneous graph structure for all observation
-   Modeling the underlying covariance matrix as a function of the covariates.
    -   The main difficulty of this approach is to enforce sparsity in the precision matrix while being positive definite

## The W-PL Approach (Brief Introduction to Pseudo-likelihood approach)

```{=tex}
\note{
 Gozde
 
 Suppose there are $n$ individuals, indexed $i = 1,..,n$. Let the $i$-th observation in the data set $X$ be denoted as $X_i = (x_{i,1},...,x_{i,p})$, which corresponds to the $i$-th individual. Let $x_{i,−j} \in \mathbb R^{p−1}$ denote the vector of the $i$-th observation including all variables except $x_{i,j}$. This approach tries to model the conditional distribution of each of the $x_j$'s given all other variables, denoted by $X_{−j} \in \mathbb R^{n \times(p−1)}$. Let the $(p − 1)$-dimensional vector $\beta_j$ indicate the regression effect on $X_{-j}$ on $x_j$. Then the conditional likelihood of $x_j$ denoted by $L(j)$ can be written as 
 \begin{equation}
L(j) = p(x_j | X_{-j}, \beta_j) \propto \prod_{i =1}^n \exp \left\{-(x_{i,j} - x_{i, -j}^T\beta_j)^2/ 2\sigma^2 \right\},
\end{equation} 
with a possibly sparse coefficient vector $\beta_j$. Then for a fixed graph $G$ the pseudo-likelihood can be calculated as 
\begin{equation}
L(G) = \prod_{j =1}^p L(j) = \prod_{j =1}^p p(x_j | X_{-j}, \beta_j). 
\end{equation}
}
```
-   Suppose there are $n$ individuals, indexed $i = 1, \dots,n$.
-   Let $X_i = (x_{i,1}, \dots, x_{i,p})$, which corresponds to the $i$-th individual.
-   Let $x_{i,−j} \in \mathbb R^{p−1}$ denote the vector of the $i$-th observation including all variables except $x_{i,j}$.
-   Model the conditional distribution of each of the $x_j$'s given all other variables, denoted by $X_{−j} \in \mathbb R^{n \times(p−1)}$.

## The W-PL Approach (Brief Introduction to Pseudo-likelihood approach)

```{=tex}
\note{
 Gozde
 
Let the $(p − 1)$-dimensional vector $\beta_j$ indicate the regression effect on $X_{-j}$ on $x_j$. Then the conditional likelihood of $x_j$ denoted by $L(j)$ can be written as 
\begin{equation}
L(j) = p(x_j | X_{-j}, \beta_j) \propto \prod_{i =1}^n \exp \left\{-(x_{i,j} - x_{i, -j}^T\beta_j)^2/ 2\sigma^2 \right\},
\end{equation} 
with a possibly sparse coefficient vector $\beta_j$. Then for a fixed graph $G$ the pseudo-likelihood can be calculated as 
\begin{equation}
L(G) = \prod_{j =1}^n L(j) = \prod_{j =1}^n p(x_j | X_{-j}, \beta_j). 
\end{equation}
}
```
-   Let the $(p − 1)$-dimensional vector $\beta_j$ indicate the regression effect on $X_{-j}$ on $x_j$.
-   The conditional likelihood of $x_j$ $$
    L(j) = p(x_j | X_{-j}, \beta_j) \propto \prod_{i =1}^n \exp \left\{-(x_{i,j} - x_{i, -j}^T\beta_j)^2/ 2\sigma^2\right\}
    $$
-   for a fixed graph $G$ the pseudo-likelihood $L(G)$ $$
    L(G) = \prod_{j =1}^n L(j) = \prod_{j =1}^n p(x_j | X_{-j}, \beta_j). 
    $$

## The W-PL Approach (Brief Introduction to Pseudo-likelihood approach)

```{=tex}
\note{
 Gozde
 
 In this paper different from the previous methods, they define a weighted version of this conditional likelihood for each individual. They assume that the underlying graph structure is a function of extraneous covariates $z$. Thus, we allow the coefficient vector $\beta_j$'s to be different for different individuals, depending on the extraneous covariates. $\beta_j^l \in \mathbb R^{p−1}$ denotes the coefficient vector corresponding to the regression of the variable $x_j$ on the remaining variables for individual $l$. Let $z_i$ denote the covariate vector associated with the $i$-th individual and define $\mathbf z = (\mathbf z_1, . . . , \mathbf z_n)$. Next, relative to the covariate $z$, we assign weights $w(z, \mathbf z_i) = \phi_{\tau} (\Vert z − \mathbf z_l\Vert)$ to every individual where $\phi_{\tau}$ is the Gaussian density with mean 0 and variance $\tau^2$. When $z = z_l$ corresponds to the $l$-th individual in the study, we use the notation $w_l(\mathbf z_i) = w(\mathbf z_l, \mathbf z_i)$ to denote the weight associated with the $i$-th individual. \\ Proposed conditional working model: for $i = 1, \dots ,n$, $$
x_{ij} | x_{i, -j}, \mathbf z, z \sim N(x_{i, -j}^T\beta_j(z), \sigma^2/ w(z,\mathbf z_i ))
$$ Next let $W(z, \mathbf z)$ denote the diagonal matrix $\text{Diag}(w(z, \mathbf z_1), \dots , w(z, \mathbf z_n))$.
}
```
-   We now define a weighted version of this conditional likelihood for each individual:
-   Underlying graph structure is a function of extraneous covariates $z$.
-   Thus, we allow the coefficient vector $\beta_j$'s to be different for different individuals, depending on the extraneous covariates $z$.
-   $\beta_j^l \in \mathbb R^{p−1}$: the coefficient vector corresponding to the regression of the variable $x_j$ on the remaining variables for individual $l$.

## The W-PL Approach

```{=tex}
\note{
 Gozde
 
Let $z_i$ denote the covariate vector associated with the $i$-th individual and define $\mathbf z = (\mathbf z_1, . . . , \mathbf z_n)$. Next, relative to the covariate $z$, we assign weights $w(z, \mathbf z_i) = \phi_{\tau} (\Vert z − \mathbf z_l\Vert)$ to every individual where $\phi_{\tau}$ is the Gaussian density with mean 0 and variance $\tau^2$. When $z = z_l$ corresponds to the $l$-th individual in the study, we use the notation $w_l(\mathbf z_i) = w(\mathbf z_l, \mathbf z_i)$ to denote the weight associated with the $i$-th individual. \\ Proposed conditional working model: for $i = 1, \dots ,n$, $$
x_{ij} | x_{i, -j}, \mathbf z, z \sim N(x_{i, -j}^T\beta_j(z), \sigma^2/ w(z,\mathbf z_i ))
$$ Next let $W(z, \mathbf z)$ denote the diagonal matrix $\text{Diag}(w(z, \mathbf z_1), \dots , w(z, \mathbf z_n))$. 
}
```
-   Let $z_i$ denote the covariate vector associated with the $i$-th individual and $\mathbf z = (\mathbf z_1,\dots , \mathbf z_n)$
-   Define weights $w(z, \mathbf z_i) = \phi_{\tau} (\Vert z − z_i\Vert)$ to every individual
-   When $z = z_l$ corresponds to the $l$-th individual, we use the notation $w_l(\mathbf z_i) = w(\mathbf z_l, \mathbf z_i)$ to denote the weight associated with the $i$-th individual.
-   **Proposed conditional working model:** for $i = 1, \dots ,n$, $$
    x_{ij} | x_{i, -j}, \mathbf z, z \sim N(x_{i, -j}^T\beta_j(z), \sigma^2/ w(z,\mathbf z_i ))
    $$

## The W-PL Approach

```{=tex}
\note{
 Gozde
 
Next let $W(z, \mathbf z)$ denote the diagonal matrix $\text{Diag}(w(z, \mathbf z_1), \dots , w(z, \mathbf z_n))$. The weighted conditional distribution function can be calculated as
\begin{equation}
p^w(x_j | X_{-j}, \beta_j(\mathbf z), \mathbf z, z) = \left(\prod_{i =1}^n \sqrt{\frac{w(z, \mathbf z_i)}{2\pi \sigma^2_*}}\right) \exp \left\{ - \frac{(x_j - X_{-j}\beta_j(z))^T W(z, \mathbf z) (x_j - X_{-j}\beta_j(z))}{2 \sigma^2_*}\right\}
\end{equation}

Then using the previous pseudo-likelihood for the graph $G$, we now give the weighted pseudo-likelihood for the graph $G(z)$ corresponding to a covariate value $z$, $$
L^w(G(z)) = \prod_{j =1}^n p^w(x_j | \beta_j(\mathbf z), X_{-j}, \mathbf z, z)
$$ Next, we put a prior distribution for the coefficient parameters corresponding to the regression problem described before. Fix an observation 
}
```
-   Let $W(z, \mathbf z)$ be the diagonal matrix $\text{Diag}(w(z, \mathbf z_1), \dots , w(z, \mathbf z_n))$.
-   The weighted conditional distribution function can be calculated as \begin{align*}
    p^w(x_j | X_{-j}, &\beta_j(\mathbf z), \mathbf z, z) = \left(\prod_{i =1}^n \sqrt{\frac{w(z, \mathbf z_i)}{2\pi \sigma^2_*}}\right) \\
    &\exp \left\{ - \frac{(x_j - X_{-j}\beta_j(z))^T W(z, \mathbf z) (x_j - X_{-j}\beta_j(z))}{2 \sigma^2_*}\right\}
    \end{align*}
-   The weighted pseudo-likelihood for the graph $G(z)$ corresponding to a covariate value $z$: $$
    L^w(G(z)) = \prod_{j =1}^n p^w(x_j | \beta_j(\mathbf z), X_{-j}, \mathbf z, z)
    $$

## Spike and Slab for Bayesian Approach

```{=tex}
\note{
 Gozde
 
 Next,we put a prior distribution for the coefficient parameters corresponding to the regression problem described before. Fix an observation $l \in \{ 1, \dots ,n\}$ and a variable $j \in \{1, \dots, p \}$. Then a spike-and-slab prior on the parameter $\beta_j^l$. So for $k \in \{1, \dots, p \}, $ $\beta_{j,k}^l$ is assumed to come from a zero-mean Gaussian density with variance component $\sigma^2\sigma^2_\beta$ with probability $\pi$ and equals to zero with probability $1 -\pi$. Let $\gamma_{j,k}^l$ be the indicator of nonzero $\beta_{j,k}^l$ and we denote it as $\gamma_{j,k}^l = 1\{\beta_{j,k}^l \neq 0\}$ which can be treated as Bernoulli random variables with a common probability of success $\pi$. Then we define $\gamma_{j}^l = (\gamma_{j,1}^l, \dots , \gamma_{j,p}^l)$ and $\Gamma^l = \{ \gamma_{j,k}^l, j = 1, \dots , p\}$. Then prior distribution for $(\beta_{j,k}^l, \gamma_{j,k}^l)$ can be written as  
$$
p_0(\beta_{j,k}^l, \gamma_{j,k}^l) = \prod_{k =1, k \neq j}^n \delta_0(\beta_{j,k}^l)^{1 - \gamma_{j,k}^l}N(0, \beta_{j,k}^l; 0, \sigma^2\sigma^2_\beta)\prod_{k =1, k \neq j}^n \pi^{\gamma_{j,k}^l}(1 -\pi)^{\gamma_{j,k}^l}.
$$ 
}
```
-   Next we want to put a prior distribution for the coefficient parameters corresponding to the regression problem described before:
-   Fix an observation $l \in \{ 1, \dots ,n\}$ and a variable $j \in \{1, \dots, p \}$: We take a spike-and-slab prior on the parameter $\beta_j^l$ for $k \in \{1, \dots, p \}$,
-   $\beta_{j,k}^l$ is assumed to come from a zero-mean Gaussian density with variance component $\sigma^2\sigma^2_\beta$ with probability $\pi$
-   And $\beta_{j,k}^l$ equals to zero with probability $1 -\pi$.

## Spike and Slab for Bayesian Approach

```{=tex}
\note{
 Gozde
 Let $\gamma_{j,k}^l$ be the indicator of nonzero $\beta_{j,k}^l$ and we denote it as $\gamma_{j,k}^l = 1\{\beta_{j,k}^l \neq 0\}$ which can be treated as Bernoulli random variables with a common probability of success $\pi$. Then we define $\gamma_{j}^l = (\gamma_{j,1}^l, \dots , \gamma_{j,p}^l)$ and $\Gamma^l = \{ \gamma_{j,k}^l, j = 1, \dots , p\}$. Then prior distribution for $(\beta_{j,k}^l, \gamma_{j,k}^l)$ can be written as  
$$
p_0(\beta_{j,k}^l, \gamma_{j,k}^l) = \prod_{k =1, k \neq j}^n \delta_0(\beta_{j,k}^l)^{1 - \gamma_{j,k}^l}N(0, \beta_{j,k}^l; 0, \sigma^2\sigma^2_\beta)\prod_{k =1, k \neq j}^n \pi^{\gamma_{j,k}^l}(1 -\pi)^{\gamma_{j,k}^l}.
$$ 
}
```
-   Let $\gamma_{j,k}^l$ be the indicator of nonzero $\beta_{j,k}^l$ and we define $\gamma_{j}^l = (\gamma_{j,1}^l, \dots , \gamma_{j,p}^l)$ and $\Gamma^l = \{ \gamma_{j,k}^l, j = 1, \dots , p\}$.

-   Prior distribution $p_0(\beta_{j,k}^l, \gamma_{j,k}^l)$ for $(\beta_{j,k}^l, \gamma_{j,k}^l)$: $$
    \prod_{k =1, k \neq j}^p \delta_0(\beta_{j,k}^l)^{1 - \gamma_{j,k}^l}N(\beta_{j,k}^l; 0, \sigma^2\sigma^2_\beta)\prod_{k =1, k \neq j}^p \pi^{\gamma_{j,k}^l}(1 -\pi)^{\gamma_{j,k}^l}.
    $$

## Calculate the posterior distribution

```{=tex}
\note{
    Renat
     
    }
```
-   The full posterior is then given by: $$
    p(\beta_{j}^l, \gamma_{j}^l | X, Z) \propto \exp \left\{ \sum_{i=1}^n \left(x_{ij} - \sum_{k\neq j} x_{ik}\beta_{j, k}^l \right)^2 w_l(\mathbf{z}_i) \right\}p_0(\beta_{j}^l, \gamma_{j}^l)
    $$

-   Unfortunately, the posterior distribution is intractable.

-   We are going to use variational inference to approximate the posterior distribution.

## CAVI Updates

```{=tex}
\note{
    Renat
    
}
```
-   Recall that the variational inference can be formulated as: $$
    (\phi^l_j)^* = \arg \min_{\phi^l_j} D_{KL} \left[ q(\beta_{j}^l, \gamma_{j}^l | \phi^l_j) || p(\beta_{j}^l, \gamma_{j}^l | X, Z) \right]
    $$

-   We use the blocked mean-field approach which assumes that the variational distribution is factorized as follows: $$
    q(\beta_{j}^l, \gamma_{j}^l | \phi^l_j) = \prod_{j\neq k} q_k(\beta_{j, k}^l, \phi^l_{j, k}; \phi^l_{j, k})
    $$

-   Further for each factor $q_k$ of the variational distribution we assume individual spike-and-slab density: \begin{align*}
    q_k(\beta_{j, k}^l, \gamma^l_{j, k}; \phi^l_{j, k}) = N(\beta_{j,k}^l;& \mu_{j, k}^l, (s^l_{j, k})^2)^{\gamma^l_{j, k}} \delta_0(\beta^l_{j, k})^{1-\gamma^l_{j, k}}\\
     &(\alpha^l_{j, k})^{\gamma^l_{j, k}} (1 - \alpha^l_{j, k})^{1-\gamma^l_{j, k}}
    \end{align*}

## CAVI Updates

```{=tex}
\note{
    Renat
    
}
```
-   Deriving the ELBO based on the assumed variational distribution and taking partial derivatives with respect to the variational parameters we obtain the following CAVI updates: \begin{align*}
      (s_{j, k}^l)^2 &= \frac{\sigma^2}{1/\sigma^2_\beta + \sum_{i=1}^n x_{ik}^2w_l(\mathbf z_i)} \\
      \mu_{j, k}^l &= \frac{(s_{j, k}^l)^2}{\sigma^2}\sum_{i= 1}^n \left\{ w_l(\mathbf z_i)x_{ik} \left(x_{ij} - \sum_{m\neq j, k}x_{im}\mu_{j, m}^l \alpha_{j,m}^l \right)  \right\} \\
      \text{logit}(\alpha_{j, k}^l) &= \text{logit}( \pi) + \left( \frac{\mu_{j, k}^l}{ \sqrt{2} s_{j, k}^l} \right)^2 + \log \frac{s_{j, k}^l}{\sigma \sigma_\beta} \\
    \end{align*}
-   The resulting inclusion probabilities $\alpha_{j, k}^l$ are symmetrized to obtain valid graphical model as: $$
    \alpha_{j, k}^l = \frac{1}{2}\left( \alpha_{j, k}^l + \alpha_{k, j}^l \right)
    $$

## The False Positive Problem

```{=tex}
\note{
 Isaac
 
 Despite this model demonstrating superior sensitivity to true dependence relations than competing methods, it suffers from lower specificity. Compared with competing methods from the `mgm` and `JGL`packages, the specificity gets substantially worse as the number of features increases. In the case of analyzing gene expression data, this could lead to worse outcomes than a less sensitive and more specific model since the cost of carrying out experiments which show a lack of a predicted relation may be very expensive. Further, validating a true but relatively weak relationship may not be desirable considering the cost. Ideally we want to increase the specificity of the model without substantially hurting the model's sensitivity and speed.
}
```
-   This model demonstrates superior sensitivity than competing methods[@covdepGE]
-   It suffers from lower specificity
    -   Compared with competing methods from the `mgm` [@mgm] and `JGL` [@JGL] packages, specificity gets substantially worse with $p$.
-   For gene expression data, this could lead to worse outcomes than a less sensitive and more specific model
    -   The cost of carrying out experiments which show a lack of a predicted relation may be very expensive
-   We want to **increase the specificity and preserve the model's sensitivity and speed**.

## The False Positive Problem

```{=tex}
\note{
Isaac

We'll look at a variety of different modifications to the algorithm in order of least to most extensive changes necessary, and use the `covdepGE` package in order to generate data for simulation studies. For some of these solutions, the package's functions will be used without modification and changes will occur outside of the inference algorithm. Otherwise, any changes to the functions will be explicitly noted.
}
```
```{=tex}
\begin{table}[H]
\centering
\begin{tabular}{||c|c|c|c|c||}
  \hline
   p & n & Package & Sensitivity (sd) & FP/graph (sd) \\ 
  \hline 
   25 & 150 & `covdepGE` & 0.80 (0.08) & 1.29 (0.98) \\ 
   25 & 150 & `JGL`      & 0.72 (0.19) & 1.96 (1.92) \\ 
   25 & 150 & `mgm`      & 0.61 (0.11) & 0.09 (0.22) \\ 
   \hline
   50 & 150 & `covdepGE` & 0.74 (0.10) & 4.36 (2.22) \\ 
   50 & 150 & `JGL`      & 0.66 (0.22) & 1.69 (1.90) \\ 
   50 & 150 & `mgm`      & 0.52 (0.10) & 0.06 (0.15) \\ 
   \hline 
   100 & 300 & `covdepGE` & 0.85 (0.06) & 18.21 (3.89) \\ 
   100 & 300 & `JGL`      & 0.74 (0.11) & 1.52 (1.20) \\ 
   100 & 300 & `mgm`      & 0.68 (0.09) & 0.02 (0.08) \\
   \hline
\end{tabular}
\caption{Comparison of existing methodologies} 
\end{table}
```
# Attempted Solutions

## Feature & Covariate Scaling

```{=tex}
\note{
    Renat

}
```
-   We believe the underlying cause of the false positive problem is having the same prior inclusion probability $\pi$ across all spike-and-slab regressions.
-   The first modification we'll make is to scale the features and covariates. The idea is that by scaling the features and covariates, we can make a common prior $\pi$ a more reasonable choice.
    -   Min-max scaling ($\sim U(0, 1)$),
    -   Standardization ($\sim N(0, 1)$).
-   Pros:
    -   Easy to implement.
    -   No changes to the inference algorithm.
-   Cons:
    -   Ad-hoc solution.

## Multiple Prior Inclusion Probabilities

\note{Renat}

-   The second approach is to allow for multiple prior inclusion probabilities $\pi$:
    -   Separate $\pi$ for each individual;
    -   Cluster and assign separate $\pi$ for each cluster.
-   Pros:
    -   Theoretically and intuitively plausible.
-   Cons:
    -   Requires additional steps (clustering, assigning $\pi$ to each cluster).
    -   May be computationally prohibitive (due to hyper-parameter search for different $\pi$'s).

# Simulations

## Normalization (Baseline)

```{r baseline-sims, include=FALSE, cache = TRUE}
# Utility function for tables
meanAndSdStr = function(x) {
  string = c(paste0(round(mean(x),2)," (", round(sd(x),2), ")"))
  return(string)
}


# Load in simulation studies

# Not doing the p100 n300 case since a single simulation
# requires over 16 hours on the cluster and many simulations
# need to be done

p = c(5, 15, 25, 50) %>% as.integer()
n = c(90, 90, 150, 150) %>% as.integer()
objects_strings = c(
  "simulation_study//p5_n90//res_p5_n90_covdepGE_20220908_215120.Rda",
  "simulation_study//p15_n90//res_p15_n90_covdepGE_20220908_215229.Rda",
  "simulation_study//p25_n150//res_p25_n150_covdepGE_20220825_121750.Rda",
  "simulation_study//p50_n150//res_p50_n150_covdepGE_20220825_090326.Rda"
  # "simulation_study//p100_n300//res_p100_n300_covdepGE_20220824_084919.Rda"
)

results_original = list()
for(sim in 1:length(objects_strings)) {
  load(objects_strings[sim])
  results_original[[sim]] = results
  rm(results)
}
sim_names_original = paste0("p", p, "_n", n)
results_original = set_names(results_original, sim_names_original)

false_positives_baseline = results_original %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  )
false_negatives_baseline = results_original %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  )

fp_baseline_str = false_positives_baseline %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Baseline FPs (sd)" = string)

fn_baseline_str = false_negatives_baseline %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Baseline FNs (sd)" = string)

fp_xtable_baseline = fp_baseline_str %>%
  cbind(p, n, .) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Normalized Z, Centered X")

fn_xtable_baseline = fn_baseline_str %>%
  cbind(p, n, .) %>%
  tibble() %>%
  xtable("False \\textbf{negatives} per sample - Normalized Z, Centered X")

combo_xtable_baseline = cbind(p, n, fp_baseline_str, fn_baseline_str) %>%
  tibble() %>% 
  xtable("False Positives and False Negatives per sample - Normalized Z, Centered X")
  
rm(results_original)
```

```{=tex}
\note{
 Isaac
 The first approach will be to use a different or additional approach to feature scaling on $X$ and/or $Z$ in order to try and make a singular prior inclusion probability more appropriate.
 Currently, the default behavior in the `covdepGE` function is to perform a columnwise Z-score Normalization on $Z$ and a columnwise 0 centering on $X$. For brevity we'll call this procedure "normalization". The baseline performance under this scheme is given below. All experiments were run under 4 different setups each having different values for $p$ and $n$, and data simulated using the `generateData` function. To assess sensitivity and specificity, we'll examine the number of false positives per sample and number of false negatives per sample across 100 replications of each simulation setup. So, in all cases lower numbers are desirable. First, we'll look at the baseline performance of the existing function with no changes to the default behavior.
}
```
-   For each covariate, apply a Z-transform
    -   $z(Z) = \frac{Z-\bar{Z}}{\sigma_Z}$
-   For each variable, center at 0
    -   $c(X) = X - \bar{X}$
-   Simulations are replicated 100 times each

```{r print-baseline-xtable, results='asis', echo=FALSE}
print(combo_xtable_baseline, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

## Min/Max Scaling

-   What if we used an alternative method of feature scaling than the default normalization?

-   Define Min-Max scaling of a vector by $f(x) = \frac{x-\min(x)}{\max(x)-\min(x)}$

-   Rescaling versus Normalization

```{=tex}
\note{
 Isaac
 
 Next, we wanted to consider a different scaling method. 
 We tried 3 situations; applying min-max scaling columnwise to both X and Z, only Z, and only X.
 Notably, a major difference between Z-score normalization and 'range scaling' is that we aren't changing the shape of the distribution. However, we do still condense the range of the variables to be within [0,1]. It's also worth noting that this approach is not robust to outliers, but for our simulation examples this isn't considered.
 
Unfortunately, applying only a Max-Min scaling to any of these setups is ineffective. When we apply this scaling to X, we do get a perfect false positive rate; but only because it fails to include any edges at all. Using the raw X and a scaled Z leads to uniformly worse sensitivity and specificity.
 
}
```
```{r min-max-only-sims, include=FALSE, cache = TRUE}
load("minmax_Z_sim.Rda")

fp_xtable_mmZ = min_max_Z_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on Z" = string) 

fn_xtable_mmZ = min_max_Z_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on Z" = string) 

rm(min_max_Z_simulation_results)

load("minmax_X_sim.Rda")

fp_xtable_mmX = min_max_X_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on X" = string) 

fn_xtable_mmX = min_max_X_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on X" = string) 

rm(min_max_X_simulation_results)

load("minmax_XZ_sim.Rda")

fp_xtable_mmXZ = min_max_XZ_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on Z, X" = string) 

fn_xtable_mmXZ = min_max_XZ_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on Z, X" = string) 

combo_xtable_fp_mm = cbind(p, n, fp_baseline_str, fp_xtable_mmZ, fp_xtable_mmX, fp_xtable_mmXZ) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Max Min Scaling only; (sd)")

combo_xtable_fn_mm = cbind(p, n, fn_baseline_str, fn_xtable_mmZ, fn_xtable_mmX, fn_xtable_mmXZ) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - Max Min Scaling only; (sd)")

rm(min_max_XZ_simulation_results)
```

------------------------------------------------------------------------

```{r print-mm-only-xtable, results='asis', echo=FALSE}
print(combo_xtable_fp_mm, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
print(combo_xtable_fn_mm, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

## Combination of Scaling & Normalization

```{=tex}
\note{
 Isaac
 
 So, what if we could get the best of both worlds?
 First do a max-min transform to scale, then do the z-transform (or mean 0 center transform in the case of X). This way we are still dealing with nice Gaussian distributions after the transformations, but gain some control over the range of X.
 
 As it turns out, this works and ended up being our best result. In particular, doing this procedure on both X and Z was best overall and resulted in slightly better specificity and little change in sensitivity
}
```
-   Can we get the best of both worlds?

-   Max-min transform followed by Z-transform

    -   $\tilde{Z} = (z \circ f)(Z)$

-   Max-min transform followed by 0-centering

    -   $\tilde{X} = (c \circ f)(X)$

```{r min-max-norm-sims, include=FALSE, cache = TRUE}
load("minmax_norm_Z_sim.Rda")

fp_xtable_mmnorm_Z = min_max_norm_Z_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N Z" = string)

fn_xtable_mmnorm_Z = min_max_norm_Z_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N Z" = string)

rm(min_max_norm_Z_simulation_results)

load("minmax_norm_X_sim.Rda")

fp_xtable_mmnorm_X = min_max_norm_X_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N X" = string)

fn_xtable_mmnorm_X = min_max_norm_X_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N X" = string)

rm(min_max_norm_X_simulation_results)

load("minmax_norm_XZ_sim.Rda")

fp_xtable_mmnorm_XZ = min_max_norm_XZ_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N Z, X" = string)

fn_xtable_mmnorm_XZ = min_max_norm_XZ_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N Z, X" = string)

rm(min_max_norm_XZ_simulation_results)

combo_xtable_fp_mm = cbind(p, n, 
                           fp_baseline_str, 
                           fp_xtable_mmnorm_Z, 
                           fp_xtable_mmnorm_X, 
                           fp_xtable_mmnorm_XZ) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Max Min Scaling + Normalization; (sd)")

combo_xtable_fn_mm = cbind(p, n, 
                           fn_baseline_str, 
                           fn_xtable_mmnorm_Z, 
                           fn_xtable_mmnorm_X, 
                           fn_xtable_mmnorm_XZ) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - Max Min Scaling + Normalization; (sd)")
```

------------------------------------------------------------------------

```{r print-mm-norm-xtable, results='asis', echo=FALSE}
print(combo_xtable_fp_mm, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
print(combo_xtable_fn_mm, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")

```

## Oracle Clustering for Multiple $\pi$

\note{Renat}

Suppose we know the true number of clusters $K$ and the true cluster assignments $C$. This is true in the simulation setting.

```{r oracle-sims, include=FALSE, cache = TRUE}
load("oracle_results.Rda")

fp_xtable_oracle = oracle_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Mean (sd)" = string) %>%
  cbind(p, n, fp_baseline_str, .) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Multiple PIP with Oracle Clustering")

fn_xtable_oracle = oracle_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Mean (sd)" = string) %>%
  cbind(p, n, fn_baseline_str, .) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - Multiple PIP with Oracle Clustering")

rm(oracle_results)
```

```{r print-oracle-xtable, results='asis', echo=FALSE}
print(fp_xtable_oracle, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
print(fn_xtable_oracle, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

## Hierarchical Clustering for Multiple $\pi$

\note{Renat}

We can also try empirical clustering approaches.

```{r clustered-sims, include=FALSE, cache = TRUE}
k_vec = c(2, 3, 6) %>% as.character()
load("cluster_results.Rda")

fp_xtable_cluster = cluster_results %>%
  map(function(x)
    map(x, function(y)
      map_dbl(y, pluck, "FP_n")
      )
  ) %>% 
  flatten() %>%
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Mean (sd)" = string) %>%
  cbind(p = rep(p, each = 3), 
        n = rep(n, each = 3), 
        clusts = rep(k_vec, 4), 
        "Baseline Mean (sd)" = rep(fp_baseline_str[[1]], each = 3), .) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Multiple PIP with Hierarchical Clustering")

fn_xtable_cluster = cluster_results %>%
  map(function(x)
    map(x, function(y)
      map_dbl(y, pluck, "FN_n")
      )
  ) %>% 
  flatten() %>%
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Mean (sd)" = string) %>%
  cbind(p = rep(p, each = 3), 
        n = rep(n, each = 3), 
        clusts = rep(k_vec, 4), 
        "Baseline Mean (sd)" = rep(fn_baseline_str[[1]], each = 3), .) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - Multiple PIP with Hierarchical Clustering")

rm(cluster_results)
```

```{r print-cluster-xtable-fp, results='asis', echo=FALSE}
print(fp_xtable_cluster, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

## Hierarchical Clustering for Multiple $\pi (cont.)$

\note{Renat}

```{r print-cluster-xtable-fn, results='asis', echo=FALSE}
print(fn_xtable_cluster, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

## Individual $\pi$ for Each Observation

-   We experimented with giving every observation its own prior inclusion probability.

-   Simulations became computationally unfeasible due to the dimension of the parameter space.

-   After multiple days of running the simulation crashed due to excessive memory consumption.

-   Based on the previous results with the oracle clustering and hierarchical clustering, likely worse.

```{=tex}
\note{
 Isaac
 What if every observation had its own $\pi$?
 We experimented with giving every observation its own prior inclusion probability by specifying the cluster mapping as $\{1,2,...,n\}$ however the simulations became computationally unfeasible due to the dimension of the parameter space being grid searched over scaling linearly with $n$. After multiple days of running the simulation crashed due to excessive memory consumption (>32GB). Based on the previous results with the oracle clustering and hierarchical clustering, and a much smaller example with $p =5, n = 10$, we believe it highly unlikely that having a different prior inclusion probability for every observation would improve the false positive rate.
}
```
# Discussions

## Why does Max/Min Alone Fail?

```{=tex}
\note{
 Isaac
 We believe that the max min scaling alone performs very badly because the assumptions of our algorithm are violated. In particular, our $X$ is not zero-mean Gaussian and we aren't correcting for this fact (and are even making it worse when we move it to always be positively centered). And since max min is not robust to outliers, and our true Z is generated from uniform distributions, rescaling is likely only going to cause us to capture less information about how the conditional dependency is scaling with Z.
}
```
-   Our assumptions are being violated:

    -   Realizations of $X$ may not quite be zero-mean Gaussian and we are not correcting for this

    -   Max-Min scaling on $X$ exacerbates this issue by constantly centering at a positive value

    -   Max-Min scaling is not robust to outliers, so our scaled $Z$ is inconsistent and the weighting function performs poorly

## Why does Max/Min + Normalization Succeed?

```{=tex}
\note{
 Isaac
 We believe that max min scaling combined with the Z-tranform on covariates and centering on variables works because we are able to control the range of $X$ without having to affect the shapes of the distributions involved. And, we are still controlling the shape of $Z$ so our weight function should perform well.
}
```
-   Doesn't require distributional changes

    -   It isn't a major change over the baseline, assumptions are unchanged

-   Gives control over the range of variables in $X$ without affecting its shape

    -   Tightening the range of $X$ may make the assumption of a common prior inclusion probability more reasonable

-   Still allows for controlling the shape of $Z$ for good weight performance, and normalization helps with outlier robustness

## Why does Multiple $\pi$ Values Fail?

```{=tex}
\note{
    Renat
}
```
-   Overfitting to the data.
    -   We see that the false negatives go down as the number of clusters goes up.
    -   On the other hand, the false positives go up as the number of clusters goes up.
    -   This is because the number of clusters is increasing the number of parameters in the model.

## Additional Experiments to Try

```{=tex}
\note{
 Isaac
 In order to verify our results, there are a few more experiments we want to try running. In particular, we want to see whether we can break the Gaussian assumption of our true data generating function. The hope is that the scaling on $X$ will improve the model's robustness to a distribution with fatter tails such as a $t$ distribution with low degrees of freedom. Similarly, we want to try adding a small percentage of 'contaminated' observations who are drawn from an unrelated, independent Gaussian distribution to the one we are trying to work with. We again hope that the additional scaling we do can help combat the effects of the bad data. Finally, we want to see if we can use the additional scaling of $Z$ to account for potential non-smoothness; for example if $Z$ was double exponentially distributed.
}
```
-   Model Mispecification

    -   Injecting non-Gaussianity into $X$ - does the scaling improve model robustness?

    -   Contaminating $X$ with an unrelated, independent Gaussian distribution - can max/min scaling help?

    -   Sharp Discontinuities in $Z$ - can max/min scaling help?

# Thank you! Questions?

## References
