---
title: "Project Results"
author: "Isaac Ray, Renat Sergazinov, Gözde Sert"
format: 
  pdf:
    include-in-header:
      - preamble.tex
      - text: |
          \addtokomafont{disposition}{\rmfamily}
    number-sections: true
    toc: true
editor: visual
bibliography: references.bib
---

```{r setup, include=FALSE}
library(covdepGE)
library(tidyverse)
library(purrr)
library(Matrix)
library(xtable)
```

# Background and Introduction

Undirected graphical models enables to model multivariate distributions. Suppose we observe a $p$-dimensional sample $x = (x_1, \dots , x_p)$ from a multivariate Gaussian distribution with a non-singular covariance matrix. Then the conditional independence structure of the distribution can be represented with a graph $G$. The graph $G = (V, E)$ is characterized by a node set $V = (1, \dots, p)$ corresponding to the $p$ variables, and an edge set $E$ such that $(i,j)\in E$ if and only if $x_i$ and $x_j$ are conditionally dependent given all other variable. The goal is to estimate the underlying graph $G$ from given $n$ i.d.d. observations $x_1, \dots , x_p$. Several methods developed under this assumption however, in practice, the observations may not be identically distributed. In this paper they suppose the variability in the graph structure across observations depending on additional covariate information.

## Current Literature and Previous Methods

There are several approaches to model heterogeneous graphs. Here we mention some of them.

```{=tex}
\begin{itemize}
\item Without using covariate information: These methods depend on the criteria of first splitting the data into homogeneous groups and sharing information withing groups
\item Adding the covariates into the mean structure of Gaussian graphical models as multiple linear regressions such that the mean is a continuous function of covariates. This approaches studied from a Bayesian perspective and a frequentist perspective. For this approach still uses the homogeneous graph structure for all observation which we do not want. 
\item Modeling the underlying covariance matrix as a function of the covariates. The main difficulty of this approach is to enforce sparsity in the precision matrix while being positive definite, as the sparsity in the covariance matrix does not normally carry to the precision matrix through matrix inversion.
\end{itemize}
```
## Proposed formulation

Let $X \in \mathbb{R}^{n \times p}$ stand for the data matrix corresponding to $n$ individuals on $p$ variables. We denote the rows $X_i \in \mathbb{R}^p$ corresponding the observation for individual $i$ and the columns $x_j \in \mathbb{R}^n$. The main goal of this paper is to learn the graph structure $G$ from a collection of $p$-variate independent samples $X_i$, as a function of some extraneous covariates $z_i$ corresponding to the samples. The only assumption on the dependence structure is that the graph parameters vary smoothly with respect to the covariates, that is, if $z_i$ and $z_j$ are similar, then the graph structure corresponding to $X_i$ and $X_j$ will be similar. In this method, a weighted pseudo-likelihood (W-PL) function to obtain a posterior distribution for the graph structure for a fixed individual, with the weights defined as a function of the covariates.

## A weighted pseudo-likelihood (W-PL) approach

First we begin with introducing the pseudo-likelihood approach. Suppose there are $n$ individuals, indexed $i = 1,..,n$. Let the $i$-th observation in the data set $X$ be denoted as $X_i = (x_{i,1},...,x_{i,p})$, which corresponds to the $i$-th individual. Let $x_{i,−j} \in \mathbb R^{p−1}$ denote the vector of the $i$-th observation including all variables except $x_{i,j}$. This approach tries to model the conditional distribution of each of the $x_j$'s given all other variables, denoted by $X_{−j} \in \mathbb R^{n \times(p−1)}$. Let the $p − 1$-dimensional vector $\beta_j$ indicate the regression effect on $X_{-j}$ on $x_j$. Then the conditional likelihood of $x_j$ denoted by $L(j)$ can be written as 
\begin{equation}
L(j) = p(x_j | X_{-j}, \beta_j) \sim \prod_{i =1}^n \exp \left\{-(x_{i,j} - x_{i, -j}^T\beta_j)^2/ 2\sigma^2 \right\},
\end{equation} 
with a possibly sparse coefficient vector $\beta_j$. Then for a fixed graph $G$ the pseudo-likelihood can be calculated as 
\begin{equation}
L(G) = \prod_{j =1}^n L(j) = \prod_{j =1}^n p(x_j | X_{-j}, \beta_j). 
\end{equation}

In this paper different from the previous methods, they define a weighted version of this conditional likelihood for each individual. They assume that the underlying graph structure is a function of extraneous covariates $z$. Thus, we allow the coefficient vector $\beta_j$'s to be different for different individuals,depending on the extraneous covariates. $\beta_j^l \in \mathbb R^{p−1}$ denotes the coefficient vector corresponding to the regression of the variable $x_j$ on the remaining variables for individual $l$. Let $z_i$ denote the covariate vector associated with the $i$-th individual and define $\mathbf z = (\mathbf z_1, . . . , \mathbf z_n)$. Next, relative to the covariate $z$, we assign weights $w(z, \mathbf z_i) = \phi_{\tau} (\Vert z − \mathbf z_l\Vert)$ to every individual where $\phi_{\tau}$ is the Gaussian density with mean 0 and variance $\tau^2$. When $z = z_l$ corresponds to the $l$-th individual in the study, we use the notation $w_l(\mathbf z_i) = w(\mathbf z_l, \mathbf z_i)$ to denote the weight associated with the $i$-th individual. 

Proposed conditional working model: for $i = 1, \dots ,n$, 
$$
x_{ij} | x_{i, -j}, \mathbf z, z \sim N(x_{i, -j}^T\beta_j(z), \sigma^2/ w(z,\mathbf z_i ))
$$ 
Next let $W(z, \mathbf z)$ denote the diagonal matrix $\text{Diag}(w(z, \mathbf z_1), \dots , w(z, \mathbf z_n))$. The weighted conditional distribution function can be calculated as

```{=tex}
\begin{equation}
p^w(x_j | X_{-j}, \beta_j(\mathbf z), \mathbf z, z) = \left(\prod_{i =1}^n \sqrt{\frac{w(z, \mathbf z_i)}{2\pi \sigma^2_*}}\right) \exp \left\{ - \frac{(x_j - X_{-j}\beta_j(z))^T W(z, \mathbf z) (x_j - X_{-j}\beta_j(z))}{2 \sigma^2_*}\right\}
\end{equation}
```
Then using the previous pseudo-likelihood for the graph $G$, we now give the weighted pseudo-likelihood for the graph $G(z)$ corresponding to a covariate value $z$, 
$$
L^w(G(z)) = \prod_{j =1}^n p^w(x_j | \beta_j(\mathbf z), X_{-j}, \mathbf z, z)
$$ 
Next, we put a prior distribution for the coefficient parameters corresponding to the regression problem described before. Fix an observation $l \in \{ 1, \dots ,n\}$ and a variable $j \in \{1, \dots, p \}$. Then a spike-and-slab prior on the parameter $\beta_j^l$. So for $k \in \{1, \dots, p \},$ $\beta_{j,k}^l$ is assumed to come from a zero-mean Gaussian density with variance component $\sigma^2\sigma^2_\beta$ with probability $\pi$ and equals to zero with probability $1 -\pi$. Let $\gamma_{j,k}^l$ be the indicator of nonzero $\beta_{j,k}^l$ and we denote it as $\gamma_{j,k}^l = 1\{\beta_{j,k}^l \neq 0\}$ which can be treated as Bernoulli random variables with a common probability of success $\pi$. Then we define $\gamma_{j}^l = (\gamma_{j,1}^l, \dots , \gamma_{j,p}^l)$ and $\Gamma^l = \{ \gamma_{j,k}^l, j = 1, \dots , p\}$. Then prior distribution for $(\beta_{j,k}^l, \gamma_{j,k}^l)$ can be written as 
$$
p_0(\beta_{j,k}^l, \gamma_{j,k}^l) = \prod_{k =1, k \neq j}^n \delta_0(\beta_{j,k}^l)^{1 - \gamma_{j,k}^l}N(0, \beta_{j,k}^l; 0, \sigma^2\sigma^2_\beta)\prod_{k =1, k \neq j}^n \pi^{\gamma_{j,k}^l}(1 -\pi)^{\gamma_{j,k}^l} .
$$ 
Then the posterior distribution for $(\beta_{j,k}^l, \gamma_{j,k}^l)$ can be calculated as 
$$
p(\beta_{j,k}^l, \gamma_{j,k}^l|X) \propto \exp \left\{- \frac{1}{2\sigma^2} \sum_{i =1}^n \left(x_{ij} - \sum_{k =1, k \neq j}^p x_{ik}\beta_{j,k}^l \right)^2 w_l(\mathbf z_i) \right\}p_0(\beta_{j,k}^l, \gamma_{j,k}^l). 
$$ 

## Block Mean Field Variational Inference

Variational inference is one of popular ways to approximate the posterior distribution. In this section first we give a brief introduction to it. Then we will connect it with our problem.\

Suppose we have a parameter of interest $\theta$ with intractable posterior distribution $p(\theta)$, an observed data vector $y$, and the variational tractable family of densities $q(\theta)$. Then we want to find the best approximating density $q^*$ in a tractable family of densities $\Gamma$ with respect to KL-divergence: 
$$
q^*(\theta) = \arg\min_{q \in \Gamma} \text{KL}(q \Vert p(\theta|y)). 
$$ 
Instead of solving the above problem we work on the evidence-lower bound(ELBO): 
$$
ELBO = \int q(\theta) \log (p(y, \theta)/ q(\theta)) \ d\theta
$$ and maximize it. For our problem the parameter of interest $\theta =(\beta_{j}^l, \gamma_{j}^l)$. We apply the block mean-field approach for the variational approximation. Let $\phi_{j, k}^l = (\alpha_{j, k}^l, \mu_{j, k}^l, (s_{j, k}^l)^2)$ be free parameters corresponding to the individuals. Then we have 

$$
q_k(\beta_j^l, \gamma_j^l; \phi_j^l) = N(\beta_{j, k}^l; \mu_{j, k}^l, (s_{j, k}^l)^2)^{\gamma_{j, k}^l}\delta_0(\beta_{j, k}^l)^{1 -\gamma_{j, k}^l}(\alpha_{j, k}^l)^{\gamma_{j, k}^l}(1 - \alpha_{j, k}^l)^{1 -\gamma_{j, k}^l}.
$$ 

Then we have the following variational parameter updates using the batch-wise updating algorithm: 

$$ 
(s_{j, k}^l)^2 = \frac{\sigma^2}{1/\sigma^2_\beta + \sum_{i=1}^n x_{ik}^2w_l(\mathbf z_i)}
$$


$$
\mu_{j, k}^l = \frac{(s_{j, k}^l)^2}{\sigma^2}\sum_{i= 1}^n \left \{ w_l(\mathbf z_i)x_{ik} \right\}
$$

$$
\mu_{j, k}^l = \frac{(s_{j, k}^l)^2}{\sigma^2}\sum_{i= 1}^n \left \{ w_l(\mathbf z_i)x_{ik}\left(x_{ij} - \sum_{m \neq j, k} x_{im}\mu_{j,m}^l\alpha_{j,m}^l \right) \right\}
$$

$$
\text{logit}(\alpha_{j, k}^l) = \text{logit}(\pi) + \frac{(\mu_{j, k}^l)^2}{2(s_{j, k}^l)^2} + \log \left( \frac{s_{j, k}^l}{\sigma\sigma_\beta}\right). 
$$

# Problem

Despite this model demonstrating superior sensitivity to true dependence relations than competing methods, it suffers from lower specificity. Compared with competing methods from the `mgm` [@mgm] and `JGL` [@JGL]packages, the specificity gets substantially worse as the number of features increases. In the case of analyzing gene expression data, this could lead to worse outcomes than a less sensitive and more specific model since the cost of carrying out experiments which show a lack of a predicted relation may be very expensive. Further, validating a true but relatively weak relationship may not be desirable considering the cost. Ideally we want to **increase the specificity of the model without substantially hurting the model's sensitivity and speed**.

# Attempted Solutions

We'll look at a variety of different modifications to the algorithm in order of least to most extensive changes necessary, and use the `covdepGE` package [@covdepGE] in order to generate data for simulation studies. For some of these solutions, the package's functions will be used without modification and changes will occur outside of the inference algorithm. Otherwise, any changes to the functions will be explicitly noted.

In general, we expect that the underlying cause of the specificity issue is due to having a common prior inclusion probability $\pi$ across every spike-and-slab regression being performed despite the varying values of $Z$ (and potentially $X$). We may expect that for certain values of $Z$, we have a different belief about whether variables in $X$ are related. We'll approach this from 2 angles; first by trying to modify our variables and covariates in such a way as to make a common $\pi$ a more appropriate choice, and then by modifying the algorithm to allow for multiple $\pi$ values to be specified either a priori or as a function $\pi(X,Z)$ through something like clustering.

## Feature Scaling Changes

The first approach will be to use a different or additional approach to feature scaling on $X$ and/or $Z$ in order to try and make a single prior inclusion probability more appropriate.

### Existing Feature Scaling through Normalization

Currently, the default behavior in the `covdepGE` function is to perform a columnwise Z-score Normalization on $Z$ and a columnwise 0 centering on $X$. For brevity we'll denote this procedure by "normalization". The baseline performance under this scheme is given below. All experiments were run under 4 different setups each having different values for $p$ and $n$, and data simulated using the `generateData` function. To assess sensitivity and specificity, we'll examine the number of false positives per sample and number of false negatives per sample across 100 replications of each simulation setup. So, in all cases lower numbers are desirable. First, we'll look at the baseline performance of the existing function with no changes to the default behavior.

```{r baseline-sims, include=FALSE, cache = TRUE}
# Utility function for tables
meanAndSdStr = function(x) {
  string = c(paste0(round(mean(x),2)," (", round(sd(x),2), ")"))
  return(string)
}


# Load in simulation studies

# Not doing the p100 n300 case since a single simulation
# requires over 16 hours on the cluster and many simulations
# need to be done

p = c(5, 15, 25, 50) %>% as.integer()
n = c(90, 90, 150, 150) %>% as.integer()
objects_strings = c(
  "simulation_study//p5_n90//res_p5_n90_covdepGE_20220908_215120.Rda",
  "simulation_study//p15_n90//res_p15_n90_covdepGE_20220908_215229.Rda",
  "simulation_study//p25_n150//res_p25_n150_covdepGE_20220825_121750.Rda",
  "simulation_study//p50_n150//res_p50_n150_covdepGE_20220825_090326.Rda"
  # "simulation_study//p100_n300//res_p100_n300_covdepGE_20220824_084919.Rda"
)

results_original = list()
for(sim in 1:length(objects_strings)) {
  load(objects_strings[sim])
  results_original[[sim]] = results
  rm(results)
}
sim_names_original = paste0("p", p, "_n", n)
results_original = set_names(results_original, sim_names_original)

false_positives_baseline = results_original %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  )
false_negatives_baseline = results_original %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  )

fp_baseline_str = false_positives_baseline %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Baseline FPs (sd)" = string)

fn_baseline_str = false_negatives_baseline %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Baseline FNs (sd)" = string)

fp_xtable_baseline = fp_baseline_str %>%
  cbind(p, n, .) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Normalized Z, Centered X")

fn_xtable_baseline = fn_baseline_str %>%
  cbind(p, n, .) %>%
  tibble() %>%
  xtable("False \\textbf{negatives} per sample - Normalized Z, Centered X")

combo_xtable_baseline = cbind(p, n, fp_baseline_str, fn_baseline_str) %>%
  tibble() %>% 
  xtable("False Positives and False Negatives per sample - Normalized Z, Centered X")

rm(results_original)
```

```{r print-baseline-xtable, results='asis', echo=FALSE}
print(combo_xtable_baseline, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

### Max-Min Feature Scaling

We try max-min scaling that puts all values in the range $[0,1]$ by subtracting the minimum value and dividing by the range. We experiment with doing separately and together for $X$ and $Z$. As we can from Tables 2-3, the max-min scaling on $Z$ alone results in worse performance. Further, whenever max-min sclaing is applied to $X$, the algorithm fails to converge, resulting in no predicted edges. This is likely due to the fact that max-min scaling alters the distribution-shape of $X$. Therefore, we do not pursue this approach further. We explore the robustness of the algorithm to the misspecification (non-normality of the data $X$) in Section 3.3.

```{r min-max-only-sims, include=FALSE, cache = TRUE}
load("minmax_Z_sim.Rda")

fp_xtable_mmZ = min_max_Z_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on Z" = string) 

fn_xtable_mmZ = min_max_Z_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on Z" = string) 

rm(min_max_Z_simulation_results)

load("minmax_X_sim.Rda")

fp_xtable_mmX = min_max_X_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on X" = string) 

fn_xtable_mmX = min_max_X_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on X" = string) 

rm(min_max_X_simulation_results)

load("minmax_XZ_sim.Rda")

fp_xtable_mmXZ = min_max_XZ_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on Z, X" = string) 

fn_xtable_mmXZ = min_max_XZ_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM on Z, X" = string) 

combo_xtable_fp_mm = cbind(p, n, fp_baseline_str, fp_xtable_mmZ, fp_xtable_mmX, fp_xtable_mmXZ) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Max Min Scaling only; (sd)")

combo_xtable_fn_mm = cbind(p, n, fn_baseline_str, fn_xtable_mmZ, fn_xtable_mmX, fn_xtable_mmXZ) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - Max Min Scaling only; (sd)")

rm(min_max_XZ_simulation_results)
```

```{r print-mm-only-xtable, results='asis', echo=FALSE}
print(combo_xtable_fp_mm, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
print(combo_xtable_fn_mm, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

### Max-Min + Normalization Feature Scaling

```{r min-max-norm-sims, include=FALSE, cache = TRUE}
load("minmax_norm_Z_sim.Rda")

fp_xtable_mmnorm_Z = min_max_norm_Z_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N Z" = string)

fn_xtable_mmnorm_Z = min_max_norm_Z_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N Z" = string)

rm(min_max_norm_Z_simulation_results)

load("minmax_norm_X_sim.Rda")

fp_xtable_mmnorm_X = min_max_norm_X_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N X" = string)

fn_xtable_mmnorm_X = min_max_norm_X_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N X" = string)

rm(min_max_norm_X_simulation_results)

load("minmax_norm_XZ_sim.Rda")

fp_xtable_mmnorm_XZ = min_max_norm_XZ_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N Z, X" = string)

fn_xtable_mmnorm_XZ = min_max_norm_XZ_simulation_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("MM/N Z, X" = string)

rm(min_max_norm_XZ_simulation_results)

combo_xtable_fp_mm = cbind(p, n, 
                           fp_baseline_str, 
                           fp_xtable_mmnorm_Z, 
                           fp_xtable_mmnorm_X, 
                           fp_xtable_mmnorm_XZ) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Max Min Scaling + Normalization; (sd)")

combo_xtable_fn_mm = cbind(p, n, 
                           fn_baseline_str, 
                           fn_xtable_mmnorm_Z, 
                           fn_xtable_mmnorm_X, 
                           fn_xtable_mmnorm_XZ) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - Max Min Scaling + Normalization; (sd)")
```

```{r print-mm-norm-xtable, results='asis', echo=FALSE}
print(combo_xtable_fp_mm, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
print(combo_xtable_fn_mm, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")

```

## Multiple Prior Inclusion Probabilities

### Oracle or Informative Prior

```{r oracle-sims, include=FALSE, cache = TRUE}
```{r oracle-sims, include=FALSE, cache = TRUE}
load("oracle_results.Rda")

fp_xtable_oracle_str = oracle_results %>%
  map(function(x)
    map_dbl(x, pluck, "FP_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Mean FP (sd)" = string)
  

fn_xtable_oracle_str = oracle_results %>%
  map(function(x)
    map_dbl(x, pluck, "FN_n")
  ) %>% 
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Mean FN (sd)" = string)

combo_xtable_oracle = 
  cbind(p, n, fp_baseline_str, fp_xtable_oracle_str, fn_baseline_str, fn_xtable_oracle_str) %>%
  tibble() %>%
  xtable(caption = "False positives \\textbf{and} negatives per sample - Multiple PIP with Oracle Clustering")

rm(oracle_results)
```

```{r print-oracle-xtable, results='asis', echo=FALSE}
print(combo_xtable_oracle, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

### Prior Inclusion through Covariate Clustering

```{r clustered-sims, include=FALSE, cache = TRUE}
k_vec = c(2, 3, 6) %>% as.character()
load("cluster_results.Rda")

fp_xtable_cluster_str = cluster_results %>%
  map(function(x)
    map(x, function(y)
      map_dbl(y, pluck, "FP_n")
      )
  ) %>% 
  flatten() %>%
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Mean FP (sd)" = string)

fn_xtable_cluster_str = cluster_results %>%
  map(function(x)
    map(x, function(y)
      map_dbl(y, pluck, "FN_n")
      )
  ) %>% 
  flatten() %>%
  map_chr(meanAndSdStr) %>% 
  data.frame(string = .) %>%
  rename("Mean FN (sd)" = string)

combo_xtable_cluster = 
  cbind(p = rep(p, each = 3), 
        n = rep(n, each = 3), 
        clusts = rep(k_vec, 4), 
        "Baseline FPs (sd)" = rep(fp_baseline_str[[1]], each = 3),
        fp_xtable_cluster_str,
        "Baseline FNs (sd)" = rep(fn_baseline_str[[1]], each = 3), 
        fn_xtable_cluster_str) %>%
  tibble() %>%
  xtable(caption = "False positives \\textbf{and} negatives per sample - Multiple PIP with Hierarchical Clustering")

rm(cluster_results)
```

```{r print-cluster-xtable, results='asis', echo=FALSE}
print(combo_xtable_cluster, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

### Prior Inclusion at the Individual Level

We experimented with giving every observation its own prior inclusion probability by specifying the cluster mapping as $\{1,2,...,n\}$ however the simulations became computationally unfeasible due to the dimension of the parameter space being grid searched over scaling linearly with $n$. After multiple days of running the simulation crashed due to excessive memory consumption (\>32GB). Based on the previous results with the oracle clustering and hierarchical clustering, and a much smaller example with $p =5, n = 10$, we believe it highly unlikely that having a different prior inclusion probability for every observation would improve the false positive rate.

## Robustness Experiments

In order to verify our results, there are a few more experiments we want to try running. In particular, we want to see whether we can break the Gaussian assumption of our true data generating function. The hope is that the scaling on $X$ will improve the model's robustness to a distribution with fatter tails such as a $t$ distribution with low degrees of freedom. Similarly, we want to try adding a small percentage of 'contaminated' observations who are drawn from an unrelated, independent Gaussian distribution to the one we are trying to work with. We again hope that the additional scaling we do can help combat the effects of the bad data. Finally, we want to see if we can use the additional scaling of $Z$ to account for potential non-smoothness.

### Contamination - Gaussian

First, we'll consider the case that a proportion of our observations' true data-generating function is just noise; that is, $X_{\textrm{contaminated}} \sim N(0, I)$. Notably, it doesn't depend on $Z$ at all.

```{r normal-contam-make-table, echo = FALSE, cache=TRUE}
perc_contam = c(5, 10, 25) %>% as.integer()
p = c(5, 15, 25) %>% as.integer()
n = c(90, 90, 150) %>% as.integer()

extractStrings = function(results, strn, colname) {
  results %>% map(function(x)
        map_dbl(x, pluck, strn)
    ) %>% 
    map_chr(meanAndSdStr) %>% 
    data.frame(string = .) %>%
    rename_with(.fn = function(x){colname})
}

load("baseline_results_bg_5pc.Rda")
load("baseline_results_bg_10pc.Rda")
load("baseline_results_bg_25pc.Rda")
baseline_results_bg = list(baseline_results_bg_5pc,
                           baseline_results_bg_10pc,
                           baseline_results_bg_25pc)
rm(baseline_results_bg_5pc, 
   baseline_results_bg_10pc, 
   baseline_results_bg_25pc)


baseline_fp_bad_gaussian = baseline_results_bg %>% 
  map_dfr(~extractStrings(.x, strn = "FP_n", colname = "Baseline Mean (sd)"))

baseline_fn_bad_gaussian = baseline_results_bg %>%
  map_dfr(~extractStrings(.x, strn = "FN_n", colname = "Baseline Mean (sd)"))

rm(baseline_results_bg)


load("mm_results_bg_5pc.Rda")
load("mm_results_bg_10pc.Rda")
load("mm_results_bg_25pc.Rda")
mm_results_bg = list(mm_results_bg_5pc,
                     mm_results_bg_10pc,
                     mm_results_bg_25pc)
rm(mm_results_bg_5pc,
   mm_results_bg_10pc,
   mm_results_bg_25pc)

mm_fp_bad_gaussian = mm_results_bg %>%
  map_dfr(~extractStrings(.x, strn = "FP_n", colname = "MM Mean (sd)"))

mm_fn_bad_gaussian = mm_results_bg %>%
  map_dfr(~extractStrings(.x, strn = "FN_n", colname = "MM Mean (sd)"))

rm(mm_results_bg)

bad_gaussian_fp_tbl = 
  cbind(p = rep(p, each = 3), 
        n = rep(n, each = 3), 
        'Perc. Corrupt' = rep(perc_contam, 3), 
        baseline_fp_bad_gaussian,
        mm_fp_bad_gaussian) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Gaussian contamination in data")

bad_gaussian_fn_tbl = 
  cbind(p = rep(p, each = 3), 
        n = rep(n, each = 3), 
        'Perc. Corrupt' = rep(perc_contam, 3), 
        baseline_fn_bad_gaussian,
        mm_fn_bad_gaussian) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - Gaussian contamination in data")
```

```{r print-bg-xtable, results='asis', echo=FALSE}
print(bad_gaussian_fp_tbl, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
print(bad_gaussian_fn_tbl, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

### Contamination - Non-Gaussian

Next, we'll consider the case that a proportion of our observations' true data-generating function is coming from a multivariate $t$ distribution with $\nu$ degrees of freedom (which we'll restrict to $\nu > 2$. When generating this data, we'll use the covariance matrix $\Sigma$ obtained from $Z$ to instead specify the *scale* matrix of our multivariate $t$ distribution, with the relationship that the true covariance matrix of these $t$ distributed variables is $\Sigma_{t} = \Sigma * \nu/(\nu-2)$

```{r nongauss-contam-make-table, echo = FALSE, cache=TRUE}
mvt_df = c(3, 6, 15) %>% as.integer()

load("baseline_results_ng_3df.Rda")
load("baseline_results_ng_6df.Rda")
load("baseline_results_ng_15df.Rda")
baseline_results_ng = list(baseline_results_ng_3df,
                           baseline_results_ng_6df,
                           baseline_results_ng_15df)
rm(baseline_results_ng_3df, 
   baseline_results_ng_6df, 
   baseline_results_ng_15df)

baseline_fp_non_gaussian = baseline_results_ng %>%
  map_dfr(~extractStrings(.x, strn = "FP_n", colname = "Baseline Mean (sd)"))

baseline_fn_non_gaussian = baseline_results_ng %>%
  map_dfr(~extractStrings(.x, strn = "FN_n", colname = "Baseline Mean (sd)"))

rm(baseline_results_ng)

load("mm_results_ng_3df.Rda")
load("mm_results_ng_6df.Rda")
load("mm_results_ng_15df.Rda")
mm_results_ng = list(mm_results_ng_3df,
                     mm_results_ng_6df,
                     mm_results_ng_15df)
rm(mm_results_ng_3df,
   mm_results_ng_6df,
   mm_results_ng_15df)

mm_fp_non_gaussian = mm_results_ng %>%
  map_dfr(~extractStrings(.x, strn = "FP_n", colname = "MM Mean (sd)"))

mm_fn_non_gaussian = mm_results_ng %>%
  map_dfr(~extractStrings(.x, strn = "FN_n", colname = "MM Mean (sd)"))

non_gaussian_fp_tbl = 
  cbind(p = rep(p, each = 3), 
        n = rep(n, each = 3), 
        't-df' = rep(mvt_df, 3), 
        baseline_fp_non_gaussian,
        mm_fp_non_gaussian) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - Multivariate-t contamination in data")

non_gaussian_fn_tbl = 
  cbind(p = rep(p, each = 3), 
        n = rep(n, each = 3), 
        't-df' = rep(mvt_df, 3), 
        baseline_fn_non_gaussian,
        mm_fn_non_gaussian) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - Multivariate-t contamination in data")
```

```{r print-ng-xtable, results='asis', echo=FALSE}
print(non_gaussian_fp_tbl, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
print(non_gaussian_fn_tbl, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```


### Non-Smooth Function of Z

Finally, we want to see what happens if the true graph structure isn't a smoothly varying function of $Z$. In particular, we'll consider the true interpolation of the precision between the first and third interval is given by $\max(\min(\tan(7.5*z),1),0)$ instead of by $\beta_0+\beta_1z$.


```{r bad-z-make-table, echo = FALSE, cache=TRUE}
load("baseline_results_bad_z.Rda")
baseline_fp_bad_z = baseline_results_bad_z %>%
  extractStrings(strn = "FP_n", colname = "Baseline Mean (sd)")

baseline_fn_bad_z = baseline_results_bad_z %>%
  extractStrings(strn = "FN_n", colname = "Baseline Mean (sd)")

rm(baseline_results_bad_z)

load("mm_results_bad_z.Rda")
mm_fp_bad_z = mm_results_bad_z %>%
  extractStrings(strn = "FP_n", colname = "MM Mean (sd)")

mm_fn_bad_z = mm_results_bad_z %>%
  extractStrings(strn = "FN_n", colname = "MM Mean (sd)")

rm(mm_results_bad_z)

bad_z_fp_tbl = 
  cbind(p = p, 
        n = n, 
        baseline_fp_bad_z,
        mm_fp_bad_z) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{positives} per sample - True precision not smoothly varying with Z")

bad_z_fn_tbl = 
  cbind(p = p, 
        n = n, 
        baseline_fn_bad_z,
        mm_fn_bad_z) %>%
  tibble() %>%
  xtable(caption = "False \\textbf{negatives} per sample - True precision not smoothly varying with Z")
```

```{r print-bz-xtable, results='asis', echo=FALSE}
print(bad_z_fp_tbl, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
print(bad_z_fn_tbl, 
      comment=FALSE, 
      include.rownames = FALSE,
      table.placement = "H")
```

## References
